{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207f4b4a-4fef-4fc3-ae80-eb1bdd2dad98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e4f9e3-6f03-4fdf-b31a-82aca1640445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import (\n",
    "    udf, size, col, avg, expr, explode, countDistinct, count, length, \n",
    "    monotonically_increasing_id, concat_ws, split, row_number, when, pandas_udf, \n",
    "    min, sqrt, desc, regexp_replace, lower, lit, regexp_extract, first, broadcast\n",
    "    )\n",
    "from pyspark.sql.window import Window\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "pkl_file_path = \"/dbfs/FileStore/clusters_stats_results_v7_all_data_percentage.pkl\"\n",
    "sample_size = 1\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d82293-3e8a-4816-a3dc-6481f1340cac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc471b8-3fb3-41e7-bd37-034c271639e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles = spark.read.parquet('/linkedin/people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ee4586-cf8f-43cc-aa0b-6e7ee0bb163f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Profiles Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d5bab5-b8ee-4425-9a42-2d6323044f05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_profiles = (\n",
    "    profiles\n",
    "\n",
    "    # get specific values\n",
    "    .withColumnRenamed(\"id\", 'profile_id')\n",
    "    .withColumn('certifications_titles', col('certifications.title'))\n",
    "    .withColumn('industry', col('current_company.industry'))\n",
    "    .withColumn('education_degree', col('education.degree'))\n",
    "    .withColumn('education_field', col('education.field'))\n",
    "    .withColumn('education_establishment', col('education.title'))\n",
    "    .withColumn('volunteer_causes', col('volunteer_experience.cause'))\n",
    "    .withColumn('other_experience', col('experience.title'))\n",
    "\n",
    "    # leave only relevant cols\n",
    "    .select(\n",
    "        \"profile_id\",\n",
    "        \"certifications_titles\",\n",
    "        \"country_code\", \n",
    "        \"industry\", \n",
    "        \"current_company:name\",\n",
    "        \"education_degree\", \n",
    "        \"education_field\", \n",
    "        \"education_establishment\",\n",
    "        \"experience\",\n",
    "        \"other_experience\",\n",
    "        \"followers\",\n",
    "        \"position\", # (thats the job title)\n",
    "        \"recommendations_count\",\n",
    "        \"volunteer_causes\"\n",
    "    )\n",
    "\n",
    "    # add experience_months col\n",
    "    # Extract the first experience's duration, then parse years and months into separate columns\n",
    "    .withColumn(\"duration_short\", col(\"experience\")[0][\"duration_short\"]) \\\n",
    "    .withColumn(\"years\", when(col(\"duration_short\").contains(\"year\"), \n",
    "                              expr(\"CAST(SPLIT(duration_short, ' ')[0] AS INT)\")).otherwise(0)) \\\n",
    "    .withColumn(\"months\", when(col(\"duration_short\").contains(\"month\"),\n",
    "                               expr(\"\"\"\n",
    "                                    CAST(SPLIT(duration_short, ' ')[\n",
    "                                        CASE\n",
    "                                            WHEN duration_short LIKE '%year%' THEN 2\n",
    "                                            ELSE 0\n",
    "                                        END\n",
    "                                    ] AS INT)\n",
    "                                \"\"\")).otherwise(0)) \\\n",
    "    .withColumn(\"experience_months\", col(\"years\") * 12 + col(\"months\")) \\\n",
    "\n",
    "    .dropna(subset=[\"position\", \"industry\", \"experience_months\"])\n",
    "    \n",
    "    # add seniority level (huristically)\n",
    "    # .withColumn(\"seniority_level\", seniority_udf(\"experience_months\"))\n",
    "    .withColumn(\"seniority_level\",\n",
    "        when((col(\"experience_months\") == 0), \"Internship\")\n",
    "        .when((col(\"experience_months\") > 0) & (col(\"experience_months\") <= 24), \"Entry Level\")\n",
    "        .when((col(\"experience_months\") > 24) & (col(\"experience_months\") <= 60), \"Associate\")\n",
    "        .when((col(\"experience_months\") > 60) & (col(\"experience_months\") <= 120), \"Mid-Senior Level\")\n",
    "        .when((col(\"experience_months\") > 120) & (col(\"experience_months\") <= 180), \"Director\")\n",
    "        .when(col(\"experience_months\") > 180, \"Executive\")\n",
    "        .otherwise(\"Not Applicable\")\n",
    "    )\n",
    "\n",
    "    # leave only relevant cols\n",
    "    .select(\n",
    "        \"profile_id\",\n",
    "        \"certifications_titles\",\n",
    "        \"country_code\", \n",
    "        \"industry\", \n",
    "        \"current_company:name\",\n",
    "        \"education_degree\", \n",
    "        \"education_field\", \n",
    "        \"education_establishment\",\n",
    "        \"other_experience\",\n",
    "        \"followers\",\n",
    "        \"position\", # (thats the job title)\n",
    "        \"recommendations_count\",\n",
    "        \"volunteer_causes\",\n",
    "        'experience_months',\n",
    "        'seniority_level'\n",
    "    )\n",
    "\n",
    "    # cache this df\n",
    "    .cache()\n",
    ")\n",
    "# for embedding: position, industry, seniority_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a11a34cd-19dc-485c-9015-0ab64a3cae4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample\n",
    "# TODO: this is temporary for debug. in the end remove and run all.\n",
    "if sample_size < 1:\n",
    "    df_profiles = df_profiles.sample(False, sample_size, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db144af3-bbd1-462f-ac1c-f5eec8b7b14c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Jobs Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f900386-4f0f-45cc-b0aa-42158128c6e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# embedding\n",
    "\n",
    "def add_embedding_to_df(df, job_title_col_name='position', industry_col_name='industry', seniority_level_col_name='seniority_level'):\n",
    "\n",
    "    # Combine the relevant columns into a single column\n",
    "    df = df.withColumn(\"combined\", concat_ws(\" \", job_title_col_name, industry_col_name, seniority_level_col_name))\n",
    "\n",
    "    # Tokenize the combined column\n",
    "    tokenizer = Tokenizer(inputCol=\"combined\", outputCol=\"tokens\")\n",
    "    df = tokenizer.transform(df)\n",
    "\n",
    "    # Create a Word2Vec model\n",
    "    word2Vec = Word2Vec(vectorSize=100, minCount=5, inputCol='tokens', outputCol='embedding')\n",
    "\n",
    "    # Cache the DataFrame as fitting the Word2Vec model is expensive\n",
    "    df.cache()\n",
    "\n",
    "    # Fit the Word2Vec model\n",
    "    model = word2Vec.fit(df)\n",
    "\n",
    "    # Transform the DataFrame to add the embeddings\n",
    "    result_df = model.transform(df)\n",
    "\n",
    "    # Don't forget to unpersist the DataFrame after the transformation\n",
    "    df.unpersist()\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201b9a50-b2f8-46dd-97f3-bfb0862227a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_profiles_emb = add_embedding_to_df(df=df_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53abd631-029e-4178-9d9e-1cd2bdb8fc8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237b90a7-f340-4b36-ba66-9085ab5879cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get centroids \n",
    "# File location and type\n",
    "path_to_centroids = \"dbfs:/FileStore/tables/jobs_clusterd.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_clusters = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(path_to_centroids) \\\n",
    "  .drop(\"embedding\") \n",
    "\n",
    "df_centroids = df_clusters\\\n",
    "  .filter(col('median') == 1)\\\n",
    "  .filter(col('cluster') != -1)\\\n",
    "  .withColumnRenamed(\"cluster\", 'centroid_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c900d586-2397-4d9b-80f2-d8af1dceafa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add embeding to centroids\n",
    "df_centroids_emb = add_embedding_to_df(df=df_centroids, job_title_col_name='job title', industry_col_name='industries', seniority_level_col_name='seniority level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3557a4b0-e548-4651-a6f5-54f3415aff3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Matching Centroids to Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7066a6ea-09bb-437e-94f3-2288f470dee2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(FloatType())\n",
    "def euclidean_distance_udf(vec1, vec2):\n",
    "    # Convert vectors to numpy arrays\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    # Calculate Euclidean distance\n",
    "    return float(np.linalg.norm(vec1 - vec2))\n",
    "\n",
    "# Assuming df_profiles_emb and df_centroids_emb have been properly generated and cached\n",
    "df_distances = (\n",
    "    df_profiles_emb.withColumnRenamed(\"id\", 'profile_id')\n",
    "    .crossJoin(broadcast(df_centroids_emb.withColumnRenamed(\"embedding\", \"embedding_centroid\").withColumnRenamed(\"id\", 'centroid_id')))\n",
    "    # Apply the UDF to calculate the Euclidean distance between the embeddings\n",
    "    .withColumn(\"euclidean_distance\", euclidean_distance_udf(\"embedding\", \"embedding_centroid\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673ea501-8b8e-46d3-8ebd-a954e31fdbf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define a window spec partitioned by profile ID (assuming \"id\" is the profile ID in df_profiles_emb) and ordered by distance\n",
    "windowSpec = Window.partitionBy(df_profiles_emb[\"profile_id\"]).orderBy(\"euclidean_distance\")\n",
    "\n",
    "# Rank centroids by distance for each profile and filter to keep only the closest centroid\n",
    "df_profiles_w_centroid = (\n",
    "    df_distances\n",
    "    .withColumn(\"rank\", row_number().over(windowSpec))\n",
    "    .filter(\"rank = 1\")\n",
    "    # .select(df_profiles_emb[\"profile_id\"], \"centroid_id\", \"euclidean_distance\")  # Adjust column names as necessary\n",
    "\n",
    "    .select(\n",
    "        \"profile_id\",\n",
    "        \"certifications_titles\",\n",
    "        \"country_code\", \n",
    "        \"industry\", \n",
    "        \"current_company:name\",\n",
    "        \"education_degree\", \n",
    "        \"education_field\", \n",
    "        \"education_establishment\",\n",
    "        \"other_experience\",\n",
    "        \"followers\",\n",
    "        \"position\", # (thats the job title)\n",
    "        \"recommendations_count\",\n",
    "        \"volunteer_causes\",\n",
    "        'experience_months',\n",
    "        'seniority_level',\n",
    "        'profile_id',\n",
    "        'centroid_id'\n",
    "    )\n",
    ")\n",
    "\n",
    "df_profiles_w_centroid = df_profiles_w_centroid.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c8ed5b-d65b-4bc3-863c-23156245df14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Statistics Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c593f3b9-14d5-4588-b408-2a8e4cd74d52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# consts\n",
    "TOP_NUM = 5\n",
    "LIST_FIELDS = [\n",
    "    'certifications_titles',\n",
    "    # 'education_degree',\n",
    "    'education_field',\n",
    "    'education_establishment',\n",
    "    'other_experience',\n",
    "    'volunteer_causes',\n",
    "]\n",
    "INT_FIELDS = [\n",
    "    'followers',\n",
    "    'recommendations_count',\n",
    "    # 'experience_months',\n",
    "]\n",
    "STR_FIELDS = [\n",
    "    'profile_id',\n",
    "    'country_code',\n",
    "    'industry',\n",
    "    'current_company:name',\n",
    "    'position',\n",
    "    'seniority_level'\n",
    "]\n",
    "\n",
    "# List of degree variations and their normalized forms\n",
    "degree_variations = [\n",
    "    (\"BS\", \"Bachelor's\"),\n",
    "    (\"Bachelor\", \"Bachelor's\"),\n",
    "    (\"M.S\", \"Master's\"),\n",
    "    (\"Master\", \"Master's\"),\n",
    "    (\"Masters\", \"Master's\"),\n",
    "    (\"M.Eng\", \"Master's\"),\n",
    "    (\"MS\", \"Master's\"),\n",
    "    (\"PhD\", \"PhD\"),\n",
    "    (\"Doctorate\", \"PhD\"),\n",
    "    (\"Doctor\", \"PhD\"),\n",
    "    (\"MBA\", \"MBA\")\n",
    "]\n",
    "# Patterns for identifying degrees\n",
    "patterns = [x[0].lower() for x in degree_variations]\n",
    "regex_pattern = \"(\" + \"|\".join(patterns) + \")\"\n",
    "\n",
    "# Define a UDF to map degrees to their normalized forms\n",
    "@udf(\"string\")\n",
    "def normalize_degree(degree):\n",
    "    degree_lower = str(degree).lower()\n",
    "    for pattern, normalized in degree_variations:\n",
    "        if pattern.lower() in degree_lower:\n",
    "            return normalized\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579f5b7b-cf8f-4e79-93d7-13942281475a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# histogram dict\n",
    "def get_histogram_dict_of_field(df, field):\n",
    "    # Collecting all follower counts\n",
    "    field_counts = df.dropna(subset=[field]).select(field).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Generating the histogram as a dictionary\n",
    "    hist_dict = {value: field_counts.count(value) for value in set(field_counts)}\n",
    "    return hist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120838e6-cfb1-4898-a82d-049a9975b042",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: DataFrame[profile_id: string, certifications_titles: array<string>, country_code: string, industry: string, current_company:name: string, education_degree: array<string>, education_field: array<string>, education_establishment: array<string>, other_experience: array<string>, followers: bigint, position: string, recommendations_count: bigint, volunteer_causes: array<string>, experience_months: int, seniority_level: string]"
     ]
    }
   ],
   "source": [
    "# Get distinct cluster_ids with minimal data movement\n",
    "clusters_ids_l = df_profiles_w_centroid.select('centroid_id').distinct().rdd.map(lambda row: row['centroid_id']).collect()\n",
    "df_profiles.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66763eb-e1af-4769-9ad4-fa5abd0ebbac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_profiles_w_centroid = df_profiles_w_centroid.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "470e0434-7c5c-49ea-b291-55bbeed846b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #  reset saved stats\n",
    "# clusters_stats_results = {}\n",
    "# with open(pkl_file_path, 'wb') as file:\n",
    "#     # Use pickle.dump to write the serialized data to the file\n",
    "#     pickle.dump(clusters_stats_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0edafde-5a58-42c0-bf46-f91004dc9157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/89 [00:00<?, ?it/s]\r  1%|          | 1/89 [00:39<58:18, 39.76s/it]\r  2%|▏         | 2/89 [01:09<49:04, 33.85s/it]\r  3%|▎         | 3/89 [01:38<45:33, 31.79s/it]\r  4%|▍         | 4/89 [02:04<41:33, 29.33s/it]\r  6%|▌         | 5/89 [02:35<41:55, 29.95s/it]\r  7%|▋         | 6/89 [03:06<41:48, 30.22s/it]\r  8%|▊         | 7/89 [03:33<40:03, 29.32s/it]\r  9%|▉         | 8/89 [04:07<41:32, 30.77s/it]\r 10%|█         | 9/89 [04:49<45:43, 34.30s/it]\r 11%|█         | 10/89 [05:17<42:43, 32.45s/it]\r 12%|█▏        | 11/89 [05:50<42:18, 32.54s/it]\r 13%|█▎        | 12/89 [06:27<43:31, 33.92s/it]\r 15%|█▍        | 13/89 [06:58<41:40, 32.90s/it]\r 16%|█▌        | 14/89 [07:26<39:15, 31.41s/it]\r 17%|█▋        | 15/89 [08:00<39:53, 32.35s/it]\r 18%|█▊        | 16/89 [08:31<38:45, 31.86s/it]\r 19%|█▉        | 17/89 [09:01<37:24, 31.17s/it]\r 20%|██        | 18/89 [09:46<41:56, 35.45s/it]\r 21%|██▏       | 19/89 [10:20<40:52, 35.04s/it]\r 22%|██▏       | 20/89 [10:46<37:18, 32.45s/it]\r 24%|██▎       | 21/89 [11:16<35:41, 31.49s/it]\r 25%|██▍       | 22/89 [11:41<33:13, 29.75s/it]\r 26%|██▌       | 23/89 [12:11<32:33, 29.60s/it]\r 27%|██▋       | 24/89 [13:00<38:37, 35.65s/it]\r 28%|██▊       | 25/89 [13:26<34:46, 32.61s/it]\r 29%|██▉       | 26/89 [14:07<36:59, 35.22s/it]\r 30%|███       | 27/89 [14:47<37:46, 36.56s/it]\r 31%|███▏      | 28/89 [15:19<35:57, 35.36s/it]\r 33%|███▎      | 29/89 [15:50<33:51, 33.86s/it]\r 34%|███▎      | 30/89 [16:37<37:13, 37.85s/it]\r 35%|███▍      | 31/89 [17:02<32:56, 34.08s/it]\r 36%|███▌      | 32/89 [17:35<31:54, 33.59s/it]\r 37%|███▋      | 33/89 [18:04<30:06, 32.25s/it]\r 38%|███▊      | 34/89 [18:40<30:34, 33.35s/it]\r 39%|███▉      | 35/89 [19:06<28:08, 31.27s/it]\r 40%|████      | 36/89 [19:39<28:05, 31.81s/it]\r 42%|████▏     | 37/89 [20:15<28:35, 32.99s/it]\r 43%|████▎     | 38/89 [20:48<28:08, 33.10s/it]\r 44%|████▍     | 39/89 [21:37<31:26, 37.72s/it]\r 45%|████▍     | 40/89 [22:05<28:29, 34.89s/it]\r 46%|████▌     | 41/89 [22:33<26:14, 32.80s/it]\r 47%|████▋     | 42/89 [22:58<23:53, 30.50s/it]\r 48%|████▊     | 43/89 [23:33<24:19, 31.73s/it]\r 49%|████▉     | 44/89 [24:02<23:19, 31.09s/it]\r 51%|█████     | 45/89 [24:27<21:29, 29.30s/it]\r 52%|█████▏    | 46/89 [24:58<21:19, 29.75s/it]\r 53%|█████▎    | 47/89 [25:29<20:58, 29.97s/it]\r 54%|█████▍    | 48/89 [25:56<19:55, 29.15s/it]\r 55%|█████▌    | 49/89 [26:23<19:00, 28.50s/it]\r 56%|█████▌    | 50/89 [26:55<19:17, 29.67s/it]\r 57%|█████▋    | 51/89 [27:28<19:24, 30.64s/it]\r 58%|█████▊    | 52/89 [28:02<19:32, 31.70s/it]\r 60%|█████▉    | 53/89 [28:44<20:47, 34.65s/it]\r 61%|██████    | 54/89 [29:10<18:45, 32.15s/it]\r 62%|██████▏   | 55/89 [29:38<17:23, 30.69s/it]\r 63%|██████▎   | 56/89 [30:07<16:42, 30.38s/it]\r 64%|██████▍   | 57/89 [30:35<15:45, 29.55s/it]\r 65%|██████▌   | 58/89 [31:09<15:59, 30.96s/it]\r 66%|██████▋   | 59/89 [31:37<15:03, 30.11s/it]\r 67%|██████▋   | 60/89 [32:20<16:23, 33.92s/it]\r 69%|██████▊   | 61/89 [32:49<15:06, 32.38s/it]\r 70%|██████▉   | 62/89 [33:17<13:58, 31.05s/it]\r 71%|███████   | 63/89 [33:45<13:02, 30.11s/it]\r 72%|███████▏  | 64/89 [34:18<12:58, 31.14s/it]\r 73%|███████▎  | 65/89 [35:03<14:08, 35.35s/it]\r 74%|███████▍  | 66/89 [35:32<12:44, 33.23s/it]\r 75%|███████▌  | 67/89 [35:59<11:29, 31.35s/it]\r 76%|███████▋  | 68/89 [36:39<11:56, 34.12s/it]\r 78%|███████▊  | 69/89 [37:12<11:13, 33.67s/it]\r 79%|███████▊  | 70/89 [37:37<09:48, 30.98s/it]\r 80%|███████▉  | 71/89 [38:02<08:45, 29.22s/it]\r 81%|████████  | 72/89 [38:27<07:55, 27.95s/it]\r 82%|████████▏ | 73/89 [38:52<07:13, 27.09s/it]\r 83%|████████▎ | 74/89 [39:17<06:37, 26.50s/it]\r 84%|████████▍ | 75/89 [39:41<06:01, 25.80s/it]\r 85%|████████▌ | 76/89 [40:06<05:33, 25.66s/it]\r 87%|████████▋ | 77/89 [40:31<05:05, 25.46s/it]\r 88%|████████▊ | 78/89 [40:56<04:37, 25.23s/it]\r 89%|████████▉ | 79/89 [41:21<04:10, 25.08s/it]\r 90%|████████▉ | 80/89 [41:45<03:43, 24.78s/it]\r 91%|█████████ | 81/89 [42:09<03:17, 24.71s/it]\r 92%|█████████▏| 82/89 [42:34<02:53, 24.81s/it]\r 93%|█████████▎| 83/89 [42:59<02:27, 24.65s/it]\r 94%|█████████▍| 84/89 [43:23<02:02, 24.53s/it]\r 96%|█████████▌| 85/89 [43:48<01:38, 24.60s/it]\r 97%|█████████▋| 86/89 [44:12<01:13, 24.56s/it]\r 98%|█████████▊| 87/89 [44:36<00:48, 24.44s/it]\r 99%|█████████▉| 88/89 [45:01<00:24, 24.44s/it]\r100%|██████████| 89/89 [45:25<00:00, 24.23s/it]\r100%|██████████| 89/89 [45:25<00:00, 30.62s/it]\n"
     ]
    }
   ],
   "source": [
    "clusters_stats_results = {}\n",
    "if os.path.isfile(pkl_file_path):\n",
    "    with open(pkl_file_path, 'rb') as file:\n",
    "        clusters_stats_results = pickle.load(file)\n",
    "\n",
    "for cluster_id in tqdm(clusters_ids_l):\n",
    "    if cluster_id in clusters_stats_results:\n",
    "        continue\n",
    "\n",
    "    df_cluster = df_profiles_w_centroid.filter(df_profiles_w_centroid['centroid_id'] == cluster_id)\n",
    "    df_cluster.cache()  # Cache the filtered DataFrame\n",
    "\n",
    "    count_similar = df_cluster.count()\n",
    "    cluster_dict = {\"CountSimilar\": count_similar}\n",
    "\n",
    "    # Use Spark SQL functions more effectively for aggregations\n",
    "    for list_field in LIST_FIELDS:\n",
    "        cluster_dict[f\"MostCommon_{list_field}\"] = (\n",
    "            df_cluster\n",
    "            .withColumn(list_field, explode(list_field))\n",
    "            .dropna(subset=[list_field])\n",
    "            .groupBy(list_field).count()\n",
    "            .orderBy(desc(\"count\"))\n",
    "            .limit(TOP_NUM)\n",
    "            .withColumn(\"percentage\", (100 * col(\"count\") / count_similar).cast(\"float\"))\n",
    "            .rdd.map(lambda row: row.asDict()).collect()\n",
    "        )\n",
    "    \n",
    "    # Explode the education_degree array into separate rows and lowercase the degree names\n",
    "    cluster_dict['MostCommon_degree_type'] = (\n",
    "        df_cluster\n",
    "        .dropna(subset=['education_degree'])\n",
    "        .withColumn(\"degree_type\", explode(\"education_degree\"))\n",
    "        .withColumn(\"normalized_degree\", normalize_degree(col(\"degree_type\")))\n",
    "        .filter(col(\"normalized_degree\").isNotNull())\n",
    "        .groupBy(\"normalized_degree\")\n",
    "        .count()\n",
    "        .orderBy(desc(\"count\"))\n",
    "        .limit(TOP_NUM)\n",
    "        .withColumn(\"percentage\", (100 * col(\"count\") / count_similar).cast(\"float\"))\n",
    "        .rdd.map(lambda row: row.asDict()).collect()\n",
    "    )\n",
    "\n",
    "    #-----------------------------------------------\n",
    "\n",
    "    for str_field in STR_FIELDS:\n",
    "        cluster_dict[f\"MostCommon_{str_field}\"] = (\n",
    "            df_cluster\n",
    "            .dropna(subset=[list_field])\n",
    "            .groupBy(str_field).count()\n",
    "            .orderBy(desc(\"count\"))\n",
    "            .limit(TOP_NUM)\n",
    "            .withColumn(\"percentage\", (100 * col(\"count\") / count_similar).cast(\"float\"))\n",
    "            .rdd.map(lambda row: row.asDict()).collect()\n",
    "        )\n",
    "\n",
    "    for int_field in INT_FIELDS:\n",
    "        cluster_dict[f\"HistDict_{int_field}\"] = get_histogram_dict_of_field(df=df_cluster, field=int_field)\n",
    "\n",
    "    # experience_years cast to years\n",
    "    df_cluster = df_cluster.withColumn(\"experience_years\", (col(\"experience_months\") / 12).cast(\"int\"))\n",
    "    cluster_dict[f\"HistDict_experience_years\"] = get_histogram_dict_of_field(df=df_cluster, field=\"experience_years\")\n",
    "\n",
    "    # add cluster dict to all dicts\n",
    "    clusters_stats_results[cluster_id] = cluster_dict\n",
    "    df_cluster.unpersist()  # Clean up cache when done with this cluster's DataFrame\n",
    "\n",
    "    with open(pkl_file_path, 'wb') as file:\n",
    "        # Use pickle.dump to write the serialized data to the file\n",
    "        pickle.dump(clusters_stats_results, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142162ec-62a8-4364-b84b-c2fd9df39df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: DataFrame[profile_id: string, certifications_titles: array<string>, country_code: string, industry: string, current_company:name: string, education_degree: array<string>, education_field: array<string>, education_establishment: array<string>, other_experience: array<string>, followers: bigint, position: string, recommendations_count: bigint, volunteer_causes: array<string>, experience_months: int, seniority_level: string, profile_id: string, centroid_id: string]"
     ]
    }
   ],
   "source": [
    "df_profiles_w_centroid.unpersist()  # Clean up the initial cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c8af42-0911-408f-9a92-e6f2bada8996",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Examine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8531ca-57c8-45a0-900a-59108d88bca0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clusters_stats_results = {}\n",
    "pkl_file_path = \"/dbfs/FileStore/clusters_stats_results_v7_all_data_percentage.pkl\"\n",
    "# pkl_file_path = \"/dbfs/FileStore/clusters_stats_results_v6_all_data.pkl\"\n",
    "if os.path.isfile(pkl_file_path):\n",
    "    with open(pkl_file_path, 'rb') as file:\n",
    "        clusters_stats_results = pickle.load(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c7af17-e182-4924-afe2-9344bd9537fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "gap=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab69fd63-f9a1-4e8e-9191-74f42d828fee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[124]: [90, 93]"
     ]
    }
   ],
   "source": [
    "i = i+gap\n",
    "[i, i+gap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6bc3349-ae1e-4ec8-9849-dd1008a05123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# c_key = '2'\n",
    "for c_key in list(clusters_stats_results.keys())[i:i+gap]:\n",
    "# for c_key in list(clusters_stats_results.keys())[87:88]:\n",
    "    print(c_key + \": \", clusters_stats_results[c_key], \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148bb7e9-a835-40db-a54a-724f0fcc43f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Project - Data Exploration",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
